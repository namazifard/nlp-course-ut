{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric, Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import DefaultDataCollator\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import metrics\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>targets</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When news is brought to one of them, of (the b...</td>\n",
       "      <td>و چون یکی از آنان را به [ولادت] دختر مژده دهند...</td>\n",
       "      <td>quran</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>After them repaired Zadok the son of Immer ove...</td>\n",
       "      <td>و چون دشمنان ما شنیدند که ما آگاه شده‌ایم و خد...</td>\n",
       "      <td>bible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>And establish regular prayers at the two ends ...</td>\n",
       "      <td>و نماز را در دو طرف روز و ساعات نخستین شب برپا...</td>\n",
       "      <td>quran</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>And it came to pass, that, when I was come aga...</td>\n",
       "      <td>و فرمود تا مدعیانش نزد تو حاضر شوند؛ و از او ب...</td>\n",
       "      <td>bible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ah woe, that Day, to the Rejecters of Truth!</td>\n",
       "      <td>وای در آن روز بر تکذیب کنندگان!</td>\n",
       "      <td>quran</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12595</th>\n",
       "      <td>Women impure are for men impure, and men impur...</td>\n",
       "      <td>زنان پلید برای مردان پلید و مردان پلید برای زن...</td>\n",
       "      <td>quran</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12596</th>\n",
       "      <td>I don't want any silly dance given in my honour.'</td>\n",
       "      <td>بنابراین حالا هم میل ندارم جشنی به افتخار من د...</td>\n",
       "      <td>mizan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12597</th>\n",
       "      <td>And the Earth will shine with the Glory of its...</td>\n",
       "      <td>و زمین به نور پروردگارش روشن می‌شود، و کتاب [ا...</td>\n",
       "      <td>quran</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12598</th>\n",
       "      <td>Then lifted I up mine eyes, and saw, and behol...</td>\n",
       "      <td>گفتم: «این چیست؟» او جواب داد: «این است آن ایف...</td>\n",
       "      <td>bible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12599</th>\n",
       "      <td>His soul was dried up.</td>\n",
       "      <td>روح خشکیده بود.</td>\n",
       "      <td>mizan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12600 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  source  \\\n",
       "0      When news is brought to one of them, of (the b...   \n",
       "1      After them repaired Zadok the son of Immer ove...   \n",
       "2      And establish regular prayers at the two ends ...   \n",
       "3      And it came to pass, that, when I was come aga...   \n",
       "4           Ah woe, that Day, to the Rejecters of Truth!   \n",
       "...                                                  ...   \n",
       "12595  Women impure are for men impure, and men impur...   \n",
       "12596  I don't want any silly dance given in my honour.'   \n",
       "12597  And the Earth will shine with the Glory of its...   \n",
       "12598  Then lifted I up mine eyes, and saw, and behol...   \n",
       "12599                             His soul was dried up.   \n",
       "\n",
       "                                                 targets category  \n",
       "0      و چون یکی از آنان را به [ولادت] دختر مژده دهند...    quran  \n",
       "1      و چون دشمنان ما شنیدند که ما آگاه شده‌ایم و خد...    bible  \n",
       "2      و نماز را در دو طرف روز و ساعات نخستین شب برپا...    quran  \n",
       "3      و فرمود تا مدعیانش نزد تو حاضر شوند؛ و از او ب...    bible  \n",
       "4                        وای در آن روز بر تکذیب کنندگان!    quran  \n",
       "...                                                  ...      ...  \n",
       "12595  زنان پلید برای مردان پلید و مردان پلید برای زن...    quran  \n",
       "12596  بنابراین حالا هم میل ندارم جشنی به افتخار من د...    mizan  \n",
       "12597  و زمین به نور پروردگارش روشن می‌شود، و کتاب [ا...    quran  \n",
       "12598  گفتم: «این چیست؟» او جواب داد: «این است آن ایف...    bible  \n",
       "12599                                    روح خشکیده بود.    mizan  \n",
       "\n",
       "[12600 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel(\"Question2_Data/train.xlsx\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['source', 'targets', 'category'],\n",
       "        num_rows: 12600\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['source', 'targets', 'category'],\n",
       "        num_rows: 2700\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['source', 'targets', 'category'],\n",
       "        num_rows: 2700\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = DatasetDict()\n",
    "for data_set in [\"train\", \"valid\", \"test\"]:\n",
    "    dataset[data_set] = Dataset.from_pandas(pd.read_excel(f\"Question2_Data/{data_set}.xlsx\"))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f36e77ce53d4626a498bb2e5fde6dcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2babe95142fa4583b079a766b269a802",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59932e0f77e5441bad26ebb51859d60a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'When news is brought to one of them, of (the birth of) a female (child), his face darkens, and he is filled with inward grief!', 'targets': 'و چون یکی از آنان را به [ولادت] دختر مژده دهند [از شدت خشم] چهره\\u200cاش سیاه گردد، ودرونش از غصه واندوه لبریز و آکنده شود!!', 'category': 'quran', 'input_ids': [0, 14847, 7123, 83, 91048, 47, 1632, 111, 2856, 4, 111, 15, 2347, 127319, 111, 16, 10, 117776, 15, 206, 38472, 247, 1919, 2577, 43334, 1755, 4, 136, 764, 83, 152382, 678, 23, 19364, 10314, 4240, 38, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'label': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: category, targets, source. If category, targets, source are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "C:\\Users\\\\Anaconda3\\lib\\site-packages\\transformers\\optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 12600\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3940\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3940' max='3940' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3940/3940 18:47, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.268000</td>\n",
       "      <td>0.150862</td>\n",
       "      <td>0.958889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.109400</td>\n",
       "      <td>0.129471</td>\n",
       "      <td>0.971481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.081300</td>\n",
       "      <td>0.173883</td>\n",
       "      <td>0.967037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.022600</td>\n",
       "      <td>0.110631</td>\n",
       "      <td>0.980000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.053800</td>\n",
       "      <td>0.124389</td>\n",
       "      <td>0.980000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to q2_parsbert\\checkpoint-500\n",
      "Configuration saved in q2_parsbert\\checkpoint-500\\config.json\n",
      "Model weights saved in q2_parsbert\\checkpoint-500\\pytorch_model.bin\n",
      "tokenizer config file saved in q2_parsbert\\checkpoint-500\\tokenizer_config.json\n",
      "Special tokens file saved in q2_parsbert\\checkpoint-500\\special_tokens_map.json\n",
      "Deleting older checkpoint [q2_parsbert\\checkpoint-3500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: category, targets, source. If category, targets, source are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2700\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to q2_parsbert\\checkpoint-1000\n",
      "Configuration saved in q2_parsbert\\checkpoint-1000\\config.json\n",
      "Model weights saved in q2_parsbert\\checkpoint-1000\\pytorch_model.bin\n",
      "tokenizer config file saved in q2_parsbert\\checkpoint-1000\\tokenizer_config.json\n",
      "Special tokens file saved in q2_parsbert\\checkpoint-1000\\special_tokens_map.json\n",
      "Deleting older checkpoint [q2_parsbert\\checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to q2_parsbert\\checkpoint-1500\n",
      "Configuration saved in q2_parsbert\\checkpoint-1500\\config.json\n",
      "Model weights saved in q2_parsbert\\checkpoint-1500\\pytorch_model.bin\n",
      "tokenizer config file saved in q2_parsbert\\checkpoint-1500\\tokenizer_config.json\n",
      "Special tokens file saved in q2_parsbert\\checkpoint-1500\\special_tokens_map.json\n",
      "Deleting older checkpoint [q2_parsbert\\checkpoint-1000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: category, targets, source. If category, targets, source are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2700\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to q2_parsbert\\checkpoint-2000\n",
      "Configuration saved in q2_parsbert\\checkpoint-2000\\config.json\n",
      "Model weights saved in q2_parsbert\\checkpoint-2000\\pytorch_model.bin\n",
      "tokenizer config file saved in q2_parsbert\\checkpoint-2000\\tokenizer_config.json\n",
      "Special tokens file saved in q2_parsbert\\checkpoint-2000\\special_tokens_map.json\n",
      "Deleting older checkpoint [q2_parsbert\\checkpoint-1500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: category, targets, source. If category, targets, source are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2700\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to q2_parsbert\\checkpoint-2500\n",
      "Configuration saved in q2_parsbert\\checkpoint-2500\\config.json\n",
      "Model weights saved in q2_parsbert\\checkpoint-2500\\pytorch_model.bin\n",
      "tokenizer config file saved in q2_parsbert\\checkpoint-2500\\tokenizer_config.json\n",
      "Special tokens file saved in q2_parsbert\\checkpoint-2500\\special_tokens_map.json\n",
      "Deleting older checkpoint [q2_parsbert\\checkpoint-2000] due to args.save_total_limit\n",
      "Saving model checkpoint to q2_parsbert\\checkpoint-3000\n",
      "Configuration saved in q2_parsbert\\checkpoint-3000\\config.json\n",
      "Model weights saved in q2_parsbert\\checkpoint-3000\\pytorch_model.bin\n",
      "tokenizer config file saved in q2_parsbert\\checkpoint-3000\\tokenizer_config.json\n",
      "Special tokens file saved in q2_parsbert\\checkpoint-3000\\special_tokens_map.json\n",
      "Deleting older checkpoint [q2_parsbert\\checkpoint-2500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: category, targets, source. If category, targets, source are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2700\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to q2_parsbert\\checkpoint-3500\n",
      "Configuration saved in q2_parsbert\\checkpoint-3500\\config.json\n",
      "Model weights saved in q2_parsbert\\checkpoint-3500\\pytorch_model.bin\n",
      "tokenizer config file saved in q2_parsbert\\checkpoint-3500\\tokenizer_config.json\n",
      "Special tokens file saved in q2_parsbert\\checkpoint-3500\\special_tokens_map.json\n",
      "Deleting older checkpoint [q2_parsbert\\checkpoint-3000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: category, targets, source. If category, targets, source are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2700\n",
      "  Batch size = 16\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3940, training_loss=0.15370832606405493, metrics={'train_runtime': 1128.6259, 'train_samples_per_second': 55.82, 'train_steps_per_second': 3.491, 'total_flos': 1082210153998176.0, 'train_loss': 0.15370832606405493, 'epoch': 5.0})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_checkpoint = \"xlm-roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=3)\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "str_to_int = {\"quran\": 0, \"bible\": 1, \"mizan\": 2}\n",
    "def tokenize_function(examples):\n",
    "    tokenized_batch = tokenizer(examples[\"source\"], truncation=True, max_length=128)\n",
    "    tokenized_batch[\"label\"] = [str_to_int[label] for label in examples[\"category\"]]\n",
    "    return tokenized_batch\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "print(tokenized_datasets[\"train\"][0])\n",
    "tokenized_datasets\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"q2_parsbert\", \n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_steps = 20,\n",
    "    learning_rate=3e-5,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    save_total_limit = 1,\n",
    "#     load_best_model_at_end=True,\n",
    "#     save_strategy = \"epoch\",\n",
    "#     metric_for_best_model=\"accuracy\",\n",
    "    group_by_length = True,\n",
    "    seed=0,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"valid\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: category, targets, source. If category, targets, source are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 2700\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: category, targets, source. If category, targets, source are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2700\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.10795055329799652, 'eval_accuracy': 0.9825925925925926, 'eval_runtime': 11.3704, 'eval_samples_per_second': 237.46, 'eval_steps_per_second': 14.863, 'epoch': 5.0}\n",
      "ID\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       quran       0.99      0.97      0.98       900\n",
      "       bible       0.97      0.99      0.98       900\n",
      "       mizan       0.98      0.98      0.98       900\n",
      "\n",
      "    accuracy                           0.98      2700\n",
      "   macro avg       0.98      0.98      0.98      2700\n",
      "weighted avg       0.98      0.98      0.98      2700\n",
      "\n",
      "AUC-ovr 0.9987145061728396\n",
      "AUC-ovo 0.9987145061728396\n"
     ]
    }
   ],
   "source": [
    "pred = trainer.predict(tokenized_datasets[\"test\"])\n",
    "print(trainer.evaluate(tokenized_datasets[\"test\"]))\n",
    "y_pred = pred.predictions.argmax(axis=-1)\n",
    "print(\"ID\")\n",
    "print(classification_report(tokenized_datasets[\"test\"][\"label\"], y_pred, target_names=str_to_int.keys()))\n",
    "print(\"AUC-ovr\", metrics.roc_auc_score(tokenized_datasets[\"test\"][\"label\"], \n",
    "                                       softmax(pred.predictions, axis=-1), multi_class=\"ovr\"))\n",
    "print(\"AUC-ovo\", metrics.roc_auc_score(tokenized_datasets[\"test\"][\"label\"], \n",
    "                                       softmax(pred.predictions, axis=-1), multi_class=\"ovo\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acc4d29438754c049602214d6af965e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a19ef3d1cf943dcb0864603ed2ccd0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d0669189d3745c38c39aa6034889a77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'When news is brought to one of them, of (the birth of) a female (child), his face darkens, and he is filled with inward grief!', 'targets': 'و چون یکی از آنان را به [ولادت] دختر مژده دهند [از شدت خشم] چهره\\u200cاش سیاه گردد، ودرونش از غصه واندوه لبریز و آکنده شود!!', 'category': 'quran', 'input_ids': [0, 65, 17980, 8583, 270, 45903, 406, 178, 378, 3606, 159045, 268, 33514, 665, 20125, 6974, 29936, 378, 8428, 46687, 76127, 376, 268, 94699, 14524, 91080, 38803, 50, 141682, 900, 870, 270, 9475, 3092, 176, 3138, 13370, 176, 43720, 103347, 65, 4573, 1901, 12157, 1994, 1146, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'label': 0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['source', 'targets', 'category', 'input_ids', 'attention_mask', 'label'],\n",
       "        num_rows: 12600\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['source', 'targets', 'category', 'input_ids', 'attention_mask', 'label'],\n",
       "        num_rows: 2700\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['source', 'targets', 'category', 'input_ids', 'attention_mask', 'label'],\n",
       "        num_rows: 2700\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    tokenized_batch = tokenizer(examples[\"targets\"], truncation=True, max_length=128)\n",
    "    tokenized_batch[\"label\"] = [str_to_int[label] for label in examples[\"category\"]]\n",
    "    return tokenized_batch\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "print(tokenized_datasets[\"train\"][0])\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: category, targets, source. If category, targets, source are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 2700\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: category, targets, source. If category, targets, source are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2700\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.515942931175232, 'eval_accuracy': 0.7925925925925926, 'eval_runtime': 12.6424, 'eval_samples_per_second': 213.567, 'eval_steps_per_second': 13.368, 'epoch': 5.0}\n",
      "OOD\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       quran       0.79      0.76      0.78       900\n",
      "       bible       0.82      0.64      0.72       900\n",
      "       mizan       0.77      0.98      0.86       900\n",
      "\n",
      "    accuracy                           0.79      2700\n",
      "   macro avg       0.80      0.79      0.79      2700\n",
      "weighted avg       0.80      0.79      0.79      2700\n",
      "\n",
      "AUC-ovr 0.9203460905349795\n",
      "AUC-ovo 0.9203460905349794\n"
     ]
    }
   ],
   "source": [
    "pred = trainer.predict(tokenized_datasets[\"test\"])\n",
    "print(trainer.evaluate(tokenized_datasets[\"test\"]))\n",
    "y_pred = pred.predictions.argmax(axis=-1)\n",
    "print(\"OOD\")\n",
    "print(classification_report(tokenized_datasets[\"test\"][\"label\"], y_pred, target_names=str_to_int.keys()))\n",
    "print(\"AUC-ovr\", metrics.roc_auc_score(tokenized_datasets[\"test\"][\"label\"], \n",
    "                                       softmax(pred.predictions, axis=-1), multi_class=\"ovr\"))\n",
    "print(\"AUC-ovo\", metrics.roc_auc_score(tokenized_datasets[\"test\"][\"label\"], \n",
    "                                       softmax(pred.predictions, axis=-1), multi_class=\"ovo\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2]), array([900, 900, 900], dtype=int64))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(tokenized_datasets[\"test\"][\"label\"], return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
